{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\n",
    "\n",
    "# Load Custom Functions\n",
    "sys.path.append('./model')\n",
    "from custom_functions import load_raw_data, extract_icd_codes, extract_dynamic_data_dict, extract_demographic_features, summarize_dynamic_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Labels.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the subsetted labels file\n",
    "labels = pd.read_csv('./data/csv/labels_original.csv')    \n",
    "stay_ids = labels['stay_id'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop for Creating all Subsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of percentages for which to create subsets\n",
    "percentages = [1, 5, 10, 25, 50, 75, 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed processing and saving for the 75% subset.\n",
      "Completed processing and saving for the 100% subset.\n"
     ]
    }
   ],
   "source": [
    "for pct in percentages:\n",
    "    # Calculate the test size based on the percentage\n",
    "    test_size = 1 - (pct / 100.0)\n",
    "    \n",
    "    # If the percentage is 100, copy the full dataset; otherwise, perform stratified sampling\n",
    "    if pct == 100:\n",
    "        subset_labels = labels.copy()\n",
    "    else:\n",
    "        subset_labels, _ = train_test_split(labels, test_size=test_size, random_state=42, stratify=labels['label'])\n",
    "    \n",
    "    # Save the subset to a new file\n",
    "    subset_file = f'./data/subsets/{pct}%_subsets/labels.csv'\n",
    "    subset_labels.to_csv(subset_file, index=False)\n",
    "    \n",
    "    # Load the subsetted labels file\n",
    "    labels_subset = pd.read_csv(subset_file)\n",
    "    stay_ids = labels_subset['stay_id'].unique()\n",
    "    \n",
    "    # Load all Time Series Files to Dictionary\n",
    "    all_data = load_raw_data(labels_subset['stay_id'], data_path='./data/csv/')\n",
    "    \n",
    "    # Use Custom Function to Create Clean Data Frame of Static Features\n",
    "    icd_code_features = extract_icd_codes(all_data)\n",
    "\n",
    "    # Save processed data to Pickle\n",
    "    icd_code_features.to_pickle(f'./data/subsets/{pct}%_subsets/icd_code_features.pkl')\n",
    "    \n",
    "    # Use Custom Function to Extract the Demographic Data Frame\n",
    "    demographic_features = extract_demographic_features(all_data)\n",
    "\n",
    "    # Save processed data to Pickle\n",
    "    demographic_features.to_pickle(f'./data/subsets/{pct}%_subsets/demographic_features.pkl')\n",
    "    \n",
    "    # Use Custom Function to Extract the Dynamic Data Dictionary\n",
    "    dynamic_data_dict = extract_dynamic_data_dict(all_data)\n",
    "    \n",
    "    # Create one Data Frame for Dynamic Data with stay_id\n",
    "    all_dynamic_values_with_id = []\n",
    "    for stay_id, df in dynamic_data_dict.items():\n",
    "        df_with_id = df.copy()\n",
    "        df_with_id['stay_id'] = stay_id\n",
    "        all_dynamic_values_with_id.append(df_with_id)\n",
    "\n",
    "    # Concatenate all dynamic data with the stay_id\n",
    "    dynamic_data_df = pd.concat(all_dynamic_values_with_id)\n",
    "\n",
    "    # Save processed data to Pickle \n",
    "    dynamic_data_df.to_pickle(f'./data/subsets/{pct}%_subsets/dynamic_data_df.pkl')\n",
    "    \n",
    "    # Extract summary statistics for dynamic data\n",
    "    sum_dynamic_features = summarize_dynamic_features(all_data)\n",
    "\n",
    "    # Save processed data to Pickle\n",
    "    sum_dynamic_features.to_pickle(f'./data/subsets/{pct}%_subsets/sum_dynamic_features.pkl')\n",
    "\n",
    "    # Print progress\n",
    "    print(f\"Completed processing and saving for the {pct}% subset.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
