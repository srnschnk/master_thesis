{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from itertools import product\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import (f1_score, accuracy_score, precision_score, \n",
    "                             recall_score, roc_auc_score, average_precision_score, \n",
    "                             confusion_matrix, precision_recall_curve, fbeta_score)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Custom Functions\n",
    "sys.path.append('./model')\n",
    "from custom_functions import load_raw_data, extract_icd_codes, extract_dynamic_data_dict, extract_demographic_features, summarize_dynamic_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Seed for all packages\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"Set seeds to make the experiment more reproducible.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, title='Confusion Matrix', cmap=plt.cm.Blues):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the percentage to load\n",
    "percentage = '1%'  # Change this to '5%', '10%', etc., as needed\n",
    "\n",
    "# Path to save tensors\n",
    "tensor_save_path = f'./data/tensors/{percentage}_subset'\n",
    "\n",
    "# Load tensors\n",
    "dynamic_train_tensor = torch.load(os.path.join(tensor_save_path, 'dynamic_train_tensor.pt'))\n",
    "static_train_tensor = torch.load(os.path.join(tensor_save_path, 'static_train_tensor.pt'))\n",
    "label_tensor_train = torch.load(os.path.join(tensor_save_path, 'label_train_tensor.pt'))\n",
    "train_tensor_repeated_static_features = torch.load(os.path.join(tensor_save_path, 'train_tensor_repeated_static_features.pt'))\n",
    "\n",
    "dynamic_test_tensor = torch.load(os.path.join(tensor_save_path, 'dynamic_test_tensor.pt'))\n",
    "static_test_tensor = torch.load(os.path.join(tensor_save_path, 'static_test_tensor.pt'))\n",
    "label_tensor_test = torch.load(os.path.join(tensor_save_path, 'label_test_tensor.pt'))\n",
    "test_tensor_repeated_static_features = torch.load(os.path.join(tensor_save_path, 'test_tensor_repeated_static_features.pt'))\n",
    "\n",
    "dynamic_val_tensor = torch.load(os.path.join(tensor_save_path, 'dynamic_val_tensor.pt'))\n",
    "static_val_tensor = torch.load(os.path.join(tensor_save_path, 'static_val_tensor.pt'))\n",
    "label_tensor_val = torch.load(os.path.join(tensor_save_path, 'label_val_tensor.pt'))\n",
    "val_tensor_repeated_static_features = torch.load(os.path.join(tensor_save_path, 'val_tensor_repeated_static_features.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorDatasets and DataLoaders\n",
    "train_dataset = TensorDataset(dynamic_train_tensor, static_train_tensor, label_tensor_train)\n",
    "test_dataset = TensorDataset(dynamic_test_tensor, static_test_tensor, label_tensor_test)\n",
    "val_dataset = TensorDataset(dynamic_val_tensor, static_val_tensor, label_tensor_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the Hybrid LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual static features in train: 1486\n",
      "Actual static features in val: 1486\n",
      "Actual static features in test: 1486\n"
     ]
    }
   ],
   "source": [
    "# Check the actual number of features in the static data tensors\n",
    "#print(\"Expected num_static_features:\", num_static_features)\n",
    "print(\"Actual static features in train:\", static_train_tensor.shape[1])\n",
    "print(\"Actual static features in val:\", static_val_tensor.shape[1])\n",
    "print(\"Actual static features in test:\", static_test_tensor.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1])\n"
     ]
    }
   ],
   "source": [
    "class HybridLSTM(nn.Module):\n",
    "    def __init__(self, num_dynamic_features, num_static_features, hidden_dim, output_dim):\n",
    "        super(HybridLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=num_dynamic_features, hidden_size=hidden_dim, batch_first=True)\n",
    "        self.static_dense = nn.Linear(num_static_features, hidden_dim)\n",
    "        self.final_dense = nn.Linear(2 * hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, dynamic_input, static_input):\n",
    "        # Dynamic pathway\n",
    "        dynamic_output, (hidden, _) = self.lstm(dynamic_input)\n",
    "        # Take the last hidden state\n",
    "        dynamic_output = dynamic_output[:, -1, :]\n",
    "\n",
    "        # Static pathway\n",
    "        static_output = self.static_dense(static_input)\n",
    "\n",
    "        # Combine outputs\n",
    "        combined_output = torch.cat((dynamic_output, static_output), dim=1)\n",
    "        final_output = self.final_dense(combined_output)\n",
    "        return self.sigmoid(final_output)\n",
    "\n",
    "# Model instantiation\n",
    "num_dynamic_features = dynamic_train_tensor.shape[2]\n",
    "num_static_features = static_train_tensor.shape[1]  \n",
    "hidden_dim = 64\n",
    "output_dim = 1  # Binary classification\n",
    "\n",
    "model = HybridLSTM(num_dynamic_features, num_static_features, hidden_dim, output_dim)\n",
    "\n",
    "# Data preparation (using your preprocessed tensors)\n",
    "train_dataset = TensorDataset(dynamic_train_tensor, static_train_tensor, label_tensor_train)\n",
    "val_dataset = TensorDataset(dynamic_val_tensor, static_val_tensor, label_tensor_val)\n",
    "test_dataset = TensorDataset(dynamic_test_tensor, static_test_tensor, label_tensor_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Forward pass\n",
    "for dynamic_data, static_data, labels in train_loader:\n",
    "    outputs = model(dynamic_data, static_data)\n",
    "    print(outputs.shape)  # Output shape check\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Training Loss: 0.5090665043819518, Validation Loss: 0.33332274556159974\n",
      "Epoch 2/100, Training Loss: 0.2434084951168015, Validation Loss: 0.3103144779801369\n",
      "Epoch 3/100, Training Loss: 0.1920491558100496, Validation Loss: 0.3041527301073074\n",
      "Epoch 4/100, Training Loss: 0.14343253345716567, Validation Loss: 0.28663143813610076\n",
      "Epoch 5/100, Training Loss: 0.09939424161400114, Validation Loss: 0.2826942682266235\n",
      "Epoch 6/100, Training Loss: 0.06487831384653137, Validation Loss: 0.27887669652700425\n",
      "Epoch 7/100, Training Loss: 0.037703375287708785, Validation Loss: 0.278685650229454\n",
      "Epoch 8/100, Training Loss: 0.021287875560422737, Validation Loss: 0.2942968159914017\n",
      "Epoch 9/100, Training Loss: 0.013077511529748639, Validation Loss: 0.30090128183364867\n",
      "Epoch 10/100, Training Loss: 0.00902057712942007, Validation Loss: 0.30968749821186065\n",
      "Epoch 00011: reducing learning rate of group 0 to 5.0000e-04.\n",
      "Epoch 11/100, Training Loss: 0.006536933429916168, Validation Loss: 0.3226257532835007\n",
      "Epoch 12/100, Training Loss: 0.005344123823479528, Validation Loss: 0.32505800426006315\n",
      "Epoch 13/100, Training Loss: 0.004781660623848438, Validation Loss: 0.32665463387966154\n",
      "Epoch 14/100, Training Loss: 0.004298412473872304, Validation Loss: 0.3295775592327118\n",
      "Epoch 00015: reducing learning rate of group 0 to 2.5000e-04.\n",
      "Epoch 15/100, Training Loss: 0.00397002262928124, Validation Loss: 0.33333480060100557\n",
      "Epoch 16/100, Training Loss: 0.0036018579544144728, Validation Loss: 0.33435803949832915\n",
      "Epoch 17/100, Training Loss: 0.003452919806087656, Validation Loss: 0.3357302397489548\n",
      "Early stopping initiated.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HybridLSTM(\n",
       "  (lstm): LSTM(835, 64, batch_first=True)\n",
       "  (static_dense): Linear(in_features=1486, out_features=64, bias=True)\n",
       "  (final_dense): Linear(in_features=128, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming 'model' is an instance of HybridLSTM\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = torch.nn.BCELoss()\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5, verbose=True)\n",
    "\n",
    "num_epochs = 100\n",
    "best_val_loss = float('inf')\n",
    "early_stopping_patience = 10\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for dynamic_input, static_input, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(dynamic_input, static_input).squeeze()\n",
    "        outputs = outputs.view(-1)  # Ensure output and labels have the same shape\n",
    "        labels = labels.view(-1)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Implement gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for dynamic_input, static_input, labels in val_loader:\n",
    "            outputs = model(dynamic_input, static_input).squeeze()\n",
    "            outputs = outputs.view(-1)  # Flatten the output for consistency\n",
    "            labels = labels.view(-1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    scheduler.step(val_loss)  # Adjust learning rate based on validation loss\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Training Loss: {total_loss / len(train_loader)}, Validation Loss: {val_loss / len(val_loader)}\")\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), './saved_models/Hybrid_LSTM/state_dict/best_hybrid_lstm_model.pth')  # Save the best model\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve == early_stopping_patience:\n",
    "            print(\"Early stopping initiated.\")\n",
    "            break\n",
    "\n",
    "# Load the best model for further use or evaluation\n",
    "model.load_state_dict(torch.load('./saved_models/Hybrid_LSTM/state_dict/best_hybrid_lstm_model.pth'))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory path\n",
    "directory_path = f'./saved_models/Hybrid_LSTM/trained_on_{percentage}'\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(directory_path, exist_ok=True)\n",
    "\n",
    "# Save model and optimizer state\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "}, os.path.join(directory_path, 'best_hybrid_LSTM_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold: 0.16\n",
      "Hybrid_LSTM Model Performance on Test Set:\n",
      "Accuracy: 0.89\n",
      "Precision: 0.42\n",
      "Recall: 0.83\n",
      "F1 Score: 0.56\n",
      "AUC-ROC: 0.94\n",
      "AUPRC: 0.54\n",
      "Performance metrics saved to: ./saved_models/Hybrid_LSTM/trained_on_1%//Hybrid_LSTM_metrics.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soere\\AppData\\Local\\Temp\\ipykernel_3312\\2681361066.py:17: RuntimeWarning: invalid value encountered in divide\n",
      "  f_beta_scores = (1 + beta**2) * (precision * recall) / ((beta**2 * precision) + recall)\n"
     ]
    }
   ],
   "source": [
    "def evaluate_hybrid_lstm_model(model, dynamic_test_tensor, static_test_tensor, label_tensor_test, directory, model_name, beta=2):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    sns.set()  # For better plot styling\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Predict on the test set\n",
    "        test_outputs = model(dynamic_test_tensor, static_test_tensor).squeeze()\n",
    "        test_probs = test_outputs.numpy()  # Probability predictions\n",
    "\n",
    "    # True labels for comparison\n",
    "    true_labels = label_tensor_test.numpy()\n",
    "\n",
    "    # Calculate precision-recall curve and corresponding thresholds\n",
    "    precision, recall, thresholds = precision_recall_curve(true_labels, test_probs)\n",
    "\n",
    "    # Calculate F-beta scores for each possible threshold\n",
    "    f_beta_scores = (1 + beta**2) * (precision * recall) / ((beta**2 * precision) + recall)\n",
    "    f_beta_scores = np.nan_to_num(f_beta_scores)  # Handling NaNs\n",
    "\n",
    "    # Find the threshold that maximizes the F-beta score\n",
    "    optimal_idx = np.argmax(f_beta_scores)\n",
    "    best_threshold = thresholds[optimal_idx]\n",
    "\n",
    "    # Use the best threshold found\n",
    "    test_predictions = (test_probs > best_threshold).astype(int)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(true_labels, test_predictions)\n",
    "    precision = precision_score(true_labels, test_predictions, zero_division=0)\n",
    "    recall = recall_score(true_labels, test_predictions)\n",
    "    f1 = f1_score(true_labels, test_predictions)\n",
    "    auc_roc = roc_auc_score(true_labels, test_probs)\n",
    "    auprc = average_precision_score(true_labels, test_probs)\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Best Threshold: {best_threshold:.2f}\")\n",
    "    print(f\"{model_name} Model Performance on Test Set:\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1 Score: {f1:.2f}\")\n",
    "    print(f\"AUC-ROC: {auc_roc:.2f}\")\n",
    "    print(f\"AUPRC: {auprc:.2f}\")\n",
    "\n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    # Save the confusion matrix\n",
    "    cm = confusion_matrix(true_labels, test_predictions)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{directory}/{model_name}_confusion_matrix.png')\n",
    "    plt.close()  # Close the plot to avoid display\n",
    "\n",
    "    # Save performance metrics to a text file\n",
    "    metrics_filepath = f'{directory}/{model_name}_metrics.txt'\n",
    "    with open(metrics_filepath, 'w') as f:\n",
    "        f.write(f\"{model_name} Model Performance on Test Set:\\n\")\n",
    "        f.write(f\"Best Threshold: {best_threshold:.2f}\\n\")\n",
    "        f.write(f\"Accuracy: {accuracy:.2f}\\n\")\n",
    "        f.write(f\"Precision: {precision:.2f}\\n\")\n",
    "        f.write(f\"Recall: {recall:.2f}\\n\")\n",
    "        f.write(f\"F1 Score: {f1:.2f}\\n\")\n",
    "        f.write(f\"AUC-ROC: {auc_roc:.2f}\\n\")\n",
    "        f.write(f\"AUPRC: {auprc:.2f}\\n\")\n",
    "\n",
    "    print(f\"Performance metrics saved to: {metrics_filepath}\")\n",
    "\n",
    "# Evaluate\n",
    "evaluate_hybrid_lstm_model(model, dynamic_test_tensor, static_test_tensor, label_tensor_test, f'./saved_models/Hybrid_LSTM/trained_on_{percentage}/', 'Hybrid_LSTM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridLSTM(nn.Module):\n",
    "    def __init__(self, num_dynamic_features, num_static_features, hidden_dim, output_dim, dropout_rate=0):\n",
    "        super(HybridLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=num_dynamic_features, hidden_size=hidden_dim, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.static_dense = nn.Linear(num_static_features, hidden_dim)\n",
    "        self.final_dense = nn.Linear(2 * hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, dynamic_input, static_input):\n",
    "        dynamic_output, (hidden, _) = self.lstm(dynamic_input)\n",
    "        dynamic_output = self.dropout(dynamic_output[:, -1, :])\n",
    "\n",
    "        static_output = self.static_dense(static_input)\n",
    "        static_output = self.dropout(static_output)\n",
    "\n",
    "        combined_output = torch.cat((dynamic_output, static_output), dim=1)\n",
    "        final_output = self.final_dense(combined_output)\n",
    "        return self.sigmoid(final_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create and return the dataloader\n",
    "def create_dataloader(batch_size):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, optimizer, criterion, scheduler, num_epochs, early_stopping_patience):\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for dynamic_input, static_input, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(dynamic_input, static_input).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        val_loss = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for dynamic_input, static_input, labels in val_loader:\n",
    "                outputs = model(dynamic_input, static_input).squeeze()\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= early_stopping_patience:\n",
    "                break\n",
    "\n",
    "    return best_val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Best Validation Loss: 0.278685650229454\n"
     ]
    }
   ],
   "source": [
    "# Define DataLoader for validation set\n",
    "val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)  # Ensure this uses the same batch size and shuffle settings as initial training\n",
    "\n",
    "# Function to compute the validation loss\n",
    "def compute_validation_loss(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for dynamic_input, static_input, labels in loader:\n",
    "            outputs = model(dynamic_input, static_input).squeeze()\n",
    "            loss = criterion(outputs, labels.view(-1))\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# Compute the initial best validation loss\n",
    "criterion = torch.nn.BCELoss()\n",
    "best_val_loss_initial = compute_validation_loss(model, val_loader, criterion)\n",
    "print(f\"Initial Best Validation Loss: {best_val_loss_initial}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue with hyperparameter tuning\n",
    "def hyperparameter_tuning(param_grid, best_val_loss_initial):\n",
    "    best_val_loss_tuned = float('inf')\n",
    "    best_params = {}\n",
    "    \n",
    "    criterion = torch.nn.BCELoss()\n",
    "\n",
    "    for hidden_dim, lr, batch_size, dropout_rate in product(param_grid['hidden_dim'], param_grid['learning_rate'], param_grid['batch_size'], param_grid['dropout_rate']):\n",
    "        print(f\"Testing configuration: Hidden Dim: {hidden_dim}, LR: {lr}, Batch Size: {batch_size}, Dropout Rate: {dropout_rate}\")\n",
    "        \n",
    "        model = HybridLSTM(num_dynamic_features, num_static_features, hidden_dim, output_dim, dropout_rate)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5, verbose=True)\n",
    "        train_loader, val_loader = create_dataloader(batch_size)\n",
    "\n",
    "        val_loss = train_model(model, train_loader, val_loader, optimizer, criterion, scheduler, 50, 5)\n",
    "\n",
    "        if val_loss < best_val_loss_tuned and val_loss < best_val_loss_initial:\n",
    "            best_val_loss_tuned = val_loss\n",
    "            best_params = {'hidden_dim': hidden_dim, 'learning_rate': lr, 'batch_size': batch_size, 'dropout_rate': dropout_rate}\n",
    "            torch.save(model.state_dict(), './saved_models/Hybrid_LSTM/state_dict/best_hybrid_lstm_model.pth')\n",
    "            print(f\"New best model saved with validation loss: {val_loss}\")\n",
    "\n",
    "    if best_params:\n",
    "        print(\"Best tuned model parameters:\", best_params)\n",
    "    else:\n",
    "        print(\"No model was better than the initial model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple grid of hyperparameters\n",
    "param_grid = {\n",
    "    'hidden_dim': [32, 64, 128],  # Increasing range\n",
    "    'learning_rate': [0.001, 0.0001, 0.00005],  # More fine-grained learning rates\n",
    "    'batch_size': [8, 16, 32],  # Wider range of batch sizes\n",
    "    'dropout_rate': [0, 0.1, 0.5]  # Adding dropout as a new parameter\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing configuration: Hidden Dim: 32, LR: 0.001, Batch Size: 8, Dropout Rate: 0\n",
      "Testing configuration: Hidden Dim: 32, LR: 0.001, Batch Size: 8, Dropout Rate: 0.1\n",
      "Testing configuration: Hidden Dim: 32, LR: 0.001, Batch Size: 8, Dropout Rate: 0.5\n",
      "Testing configuration: Hidden Dim: 32, LR: 0.001, Batch Size: 16, Dropout Rate: 0\n",
      "Testing configuration: Hidden Dim: 32, LR: 0.001, Batch Size: 16, Dropout Rate: 0.1\n",
      "Testing configuration: Hidden Dim: 32, LR: 0.001, Batch Size: 16, Dropout Rate: 0.5\n",
      "Testing configuration: Hidden Dim: 32, LR: 0.001, Batch Size: 32, Dropout Rate: 0\n",
      "Testing configuration: Hidden Dim: 32, LR: 0.001, Batch Size: 32, Dropout Rate: 0.1\n",
      "Testing configuration: Hidden Dim: 32, LR: 0.001, Batch Size: 32, Dropout Rate: 0.5\n",
      "Testing configuration: Hidden Dim: 32, LR: 0.0001, Batch Size: 8, Dropout Rate: 0\n",
      "Testing configuration: Hidden Dim: 32, LR: 0.0001, Batch Size: 8, Dropout Rate: 0.1\n",
      "Testing configuration: Hidden Dim: 32, LR: 0.0001, Batch Size: 8, Dropout Rate: 0.5\n",
      "Testing configuration: Hidden Dim: 32, LR: 0.0001, Batch Size: 16, Dropout Rate: 0\n",
      "Testing configuration: Hidden Dim: 32, LR: 0.0001, Batch Size: 16, Dropout Rate: 0.1\n",
      "Testing configuration: Hidden Dim: 32, LR: 0.0001, Batch Size: 16, Dropout Rate: 0.5\n",
      "Testing configuration: Hidden Dim: 32, LR: 0.0001, Batch Size: 32, Dropout Rate: 0\n",
      "Testing configuration: Hidden Dim: 32, LR: 0.0001, Batch Size: 32, Dropout Rate: 0.1\n",
      "Testing configuration: Hidden Dim: 32, LR: 0.0001, Batch Size: 32, Dropout Rate: 0.5\n",
      "Testing configuration: Hidden Dim: 32, LR: 5e-05, Batch Size: 8, Dropout Rate: 0\n",
      "Testing configuration: Hidden Dim: 32, LR: 5e-05, Batch Size: 8, Dropout Rate: 0.1\n",
      "Testing configuration: Hidden Dim: 32, LR: 5e-05, Batch Size: 8, Dropout Rate: 0.5\n",
      "Testing configuration: Hidden Dim: 32, LR: 5e-05, Batch Size: 16, Dropout Rate: 0\n",
      "Testing configuration: Hidden Dim: 32, LR: 5e-05, Batch Size: 16, Dropout Rate: 0.1\n",
      "Testing configuration: Hidden Dim: 32, LR: 5e-05, Batch Size: 16, Dropout Rate: 0.5\n",
      "Testing configuration: Hidden Dim: 32, LR: 5e-05, Batch Size: 32, Dropout Rate: 0\n",
      "Testing configuration: Hidden Dim: 32, LR: 5e-05, Batch Size: 32, Dropout Rate: 0.1\n",
      "Testing configuration: Hidden Dim: 32, LR: 5e-05, Batch Size: 32, Dropout Rate: 0.5\n",
      "Testing configuration: Hidden Dim: 64, LR: 0.001, Batch Size: 8, Dropout Rate: 0\n",
      "Testing configuration: Hidden Dim: 64, LR: 0.001, Batch Size: 8, Dropout Rate: 0.1\n",
      "Testing configuration: Hidden Dim: 64, LR: 0.001, Batch Size: 8, Dropout Rate: 0.5\n",
      "Testing configuration: Hidden Dim: 64, LR: 0.001, Batch Size: 16, Dropout Rate: 0\n",
      "Testing configuration: Hidden Dim: 64, LR: 0.001, Batch Size: 16, Dropout Rate: 0.1\n",
      "Testing configuration: Hidden Dim: 64, LR: 0.001, Batch Size: 16, Dropout Rate: 0.5\n",
      "Testing configuration: Hidden Dim: 64, LR: 0.001, Batch Size: 32, Dropout Rate: 0\n",
      "Testing configuration: Hidden Dim: 64, LR: 0.001, Batch Size: 32, Dropout Rate: 0.1\n",
      "Testing configuration: Hidden Dim: 64, LR: 0.001, Batch Size: 32, Dropout Rate: 0.5\n",
      "Testing configuration: Hidden Dim: 64, LR: 0.0001, Batch Size: 8, Dropout Rate: 0\n",
      "Testing configuration: Hidden Dim: 64, LR: 0.0001, Batch Size: 8, Dropout Rate: 0.1\n",
      "Testing configuration: Hidden Dim: 64, LR: 0.0001, Batch Size: 8, Dropout Rate: 0.5\n",
      "Testing configuration: Hidden Dim: 64, LR: 0.0001, Batch Size: 16, Dropout Rate: 0\n",
      "Testing configuration: Hidden Dim: 64, LR: 0.0001, Batch Size: 16, Dropout Rate: 0.1\n",
      "Testing configuration: Hidden Dim: 64, LR: 0.0001, Batch Size: 16, Dropout Rate: 0.5\n",
      "Testing configuration: Hidden Dim: 64, LR: 0.0001, Batch Size: 32, Dropout Rate: 0\n",
      "Testing configuration: Hidden Dim: 64, LR: 0.0001, Batch Size: 32, Dropout Rate: 0.1\n",
      "Testing configuration: Hidden Dim: 64, LR: 0.0001, Batch Size: 32, Dropout Rate: 0.5\n",
      "Testing configuration: Hidden Dim: 64, LR: 5e-05, Batch Size: 8, Dropout Rate: 0\n",
      "Testing configuration: Hidden Dim: 64, LR: 5e-05, Batch Size: 8, Dropout Rate: 0.1\n",
      "Testing configuration: Hidden Dim: 64, LR: 5e-05, Batch Size: 8, Dropout Rate: 0.5\n",
      "Testing configuration: Hidden Dim: 64, LR: 5e-05, Batch Size: 16, Dropout Rate: 0\n",
      "Testing configuration: Hidden Dim: 64, LR: 5e-05, Batch Size: 16, Dropout Rate: 0.1\n",
      "Testing configuration: Hidden Dim: 64, LR: 5e-05, Batch Size: 16, Dropout Rate: 0.5\n",
      "Testing configuration: Hidden Dim: 64, LR: 5e-05, Batch Size: 32, Dropout Rate: 0\n",
      "Testing configuration: Hidden Dim: 64, LR: 5e-05, Batch Size: 32, Dropout Rate: 0.1\n",
      "Testing configuration: Hidden Dim: 64, LR: 5e-05, Batch Size: 32, Dropout Rate: 0.5\n",
      "Testing configuration: Hidden Dim: 128, LR: 0.001, Batch Size: 8, Dropout Rate: 0\n",
      "Testing configuration: Hidden Dim: 128, LR: 0.001, Batch Size: 8, Dropout Rate: 0.1\n",
      "Testing configuration: Hidden Dim: 128, LR: 0.001, Batch Size: 8, Dropout Rate: 0.5\n",
      "Testing configuration: Hidden Dim: 128, LR: 0.001, Batch Size: 16, Dropout Rate: 0\n",
      "Testing configuration: Hidden Dim: 128, LR: 0.001, Batch Size: 16, Dropout Rate: 0.1\n",
      "Testing configuration: Hidden Dim: 128, LR: 0.001, Batch Size: 16, Dropout Rate: 0.5\n",
      "Testing configuration: Hidden Dim: 128, LR: 0.001, Batch Size: 32, Dropout Rate: 0\n",
      "Testing configuration: Hidden Dim: 128, LR: 0.001, Batch Size: 32, Dropout Rate: 0.1\n",
      "Testing configuration: Hidden Dim: 128, LR: 0.001, Batch Size: 32, Dropout Rate: 0.5\n",
      "Testing configuration: Hidden Dim: 128, LR: 0.0001, Batch Size: 8, Dropout Rate: 0\n",
      "Testing configuration: Hidden Dim: 128, LR: 0.0001, Batch Size: 8, Dropout Rate: 0.1\n",
      "Testing configuration: Hidden Dim: 128, LR: 0.0001, Batch Size: 8, Dropout Rate: 0.5\n",
      "Testing configuration: Hidden Dim: 128, LR: 0.0001, Batch Size: 16, Dropout Rate: 0\n",
      "Testing configuration: Hidden Dim: 128, LR: 0.0001, Batch Size: 16, Dropout Rate: 0.1\n",
      "Testing configuration: Hidden Dim: 128, LR: 0.0001, Batch Size: 16, Dropout Rate: 0.5\n",
      "Testing configuration: Hidden Dim: 128, LR: 0.0001, Batch Size: 32, Dropout Rate: 0\n",
      "Testing configuration: Hidden Dim: 128, LR: 0.0001, Batch Size: 32, Dropout Rate: 0.1\n",
      "Testing configuration: Hidden Dim: 128, LR: 0.0001, Batch Size: 32, Dropout Rate: 0.5\n",
      "Testing configuration: Hidden Dim: 128, LR: 5e-05, Batch Size: 8, Dropout Rate: 0\n",
      "Testing configuration: Hidden Dim: 128, LR: 5e-05, Batch Size: 8, Dropout Rate: 0.1\n",
      "Testing configuration: Hidden Dim: 128, LR: 5e-05, Batch Size: 8, Dropout Rate: 0.5\n",
      "Testing configuration: Hidden Dim: 128, LR: 5e-05, Batch Size: 16, Dropout Rate: 0\n",
      "Testing configuration: Hidden Dim: 128, LR: 5e-05, Batch Size: 16, Dropout Rate: 0.1\n",
      "Testing configuration: Hidden Dim: 128, LR: 5e-05, Batch Size: 16, Dropout Rate: 0.5\n",
      "Testing configuration: Hidden Dim: 128, LR: 5e-05, Batch Size: 32, Dropout Rate: 0\n",
      "Testing configuration: Hidden Dim: 128, LR: 5e-05, Batch Size: 32, Dropout Rate: 0.1\n",
      "Testing configuration: Hidden Dim: 128, LR: 5e-05, Batch Size: 32, Dropout Rate: 0.5\n",
      "No model was better than the initial model.\n"
     ]
    }
   ],
   "source": [
    "# Start hyperparameter tuning\n",
    "hyperparameter_tuning(param_grid, best_val_loss_initial)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
