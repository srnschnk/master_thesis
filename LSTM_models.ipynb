{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score, \n",
    "                             f1_score, roc_auc_score, average_precision_score, \n",
    "                             confusion_matrix, precision_recall_curve, fbeta_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Custom Functions\n",
    "sys.path.append('./model')\n",
    "from custom_functions import load_raw_data, extract_icd_codes, extract_dynamic_data_dict, extract_demographic_features, summarize_dynamic_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seeds for all libraries\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)  # Python random module\n",
    "    np.random.seed(seed)  # Numpy module\n",
    "    torch.manual_seed(seed)  # Torch\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, title='Confusion Matrix', cmap=plt.cm.Blues):\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the percentage to load\n",
    "percentage = '25%'  # '5%', '10%', etc., as needed\n",
    "\n",
    "# Path to save tensors\n",
    "tensor_save_path = f'./data/tensors/{percentage}_subset'\n",
    "\n",
    "# Load tensors\n",
    "dynamic_train_tensor = torch.load(os.path.join(tensor_save_path, 'dynamic_train_tensor.pt'))\n",
    "static_train_tensor = torch.load(os.path.join(tensor_save_path, 'static_train_tensor.pt'))\n",
    "label_train_tensor = torch.load(os.path.join(tensor_save_path, 'label_train_tensor.pt'))\n",
    "train_tensor_repeated_static_features = torch.load(os.path.join(tensor_save_path, 'train_tensor_repeated_static_features.pt'))\n",
    "\n",
    "dynamic_test_tensor = torch.load(os.path.join(tensor_save_path, 'dynamic_test_tensor.pt'))\n",
    "static_test_tensor = torch.load(os.path.join(tensor_save_path, 'static_test_tensor.pt'))\n",
    "label_test_tensor = torch.load(os.path.join(tensor_save_path, 'label_test_tensor.pt'))\n",
    "test_tensor_repeated_static_features = torch.load(os.path.join(tensor_save_path, 'test_tensor_repeated_static_features.pt'))\n",
    "\n",
    "dynamic_val_tensor = torch.load(os.path.join(tensor_save_path, 'dynamic_val_tensor.pt'))\n",
    "static_val_tensor = torch.load(os.path.join(tensor_save_path, 'static_val_tensor.pt'))\n",
    "label_val_tensor = torch.load(os.path.join(tensor_save_path, 'label_val_tensor.pt'))\n",
    "val_tensor_repeated_static_features = torch.load(os.path.join(tensor_save_path, 'val_tensor_repeated_static_features.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dynamic TensorDatasets and DataLoaders\n",
    "dynamic_train_dataset = TensorDataset(dynamic_train_tensor, label_train_tensor)\n",
    "dynamic_test_dataset = TensorDataset(dynamic_test_tensor, label_test_tensor)\n",
    "dynamic_val_dataset = TensorDataset(dynamic_val_tensor, label_val_tensor)\n",
    "\n",
    "dynamic_train_loader = DataLoader(dynamic_train_dataset, batch_size=16, shuffle=True, num_workers=4)\n",
    "dynamic_test_loader = DataLoader(dynamic_test_dataset, batch_size=16, shuffle=False, num_workers=4)\n",
    "dynamic_val_loader = DataLoader(dynamic_val_dataset, batch_size=16, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorDataset and DataLoaders for model with repeating static features\n",
    "repeated_static_features_train_dataset = TensorDataset(train_tensor_repeated_static_features, label_train_tensor)\n",
    "repeated_static_features_test_dataset = TensorDataset(test_tensor_repeated_static_features, label_test_tensor)\n",
    "repeated_static_features_val_dataset = TensorDataset(val_tensor_repeated_static_features, label_val_tensor)\n",
    "\n",
    "repeated_static_features_train_loader = DataLoader(repeated_static_features_train_dataset, batch_size=16, shuffle=True, num_workers=4)\n",
    "repeated_static_features_test_loader = DataLoader(repeated_static_features_test_dataset, batch_size=16, shuffle=False, num_workers=4)\n",
    "repeated_static_features_val_loader = DataLoader(repeated_static_features_val_dataset, batch_size=16, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, num_time_steps, num_dynamic_features):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=num_dynamic_features, hidden_size=64, batch_first=True, dropout=0.5)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.output = nn.Linear(64, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, dynamic_input):\n",
    "        lstm_out, _ = self.lstm(dynamic_input)\n",
    "        lstm_out = self.dropout(lstm_out[:, -1, :])\n",
    "        x = self.output(lstm_out)\n",
    "        return self.sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train LSTM on only Dynamic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soere\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "# Model instantiation\n",
    "num_time_steps = dynamic_train_tensor.shape[1]\n",
    "num_dynamic_features = dynamic_train_tensor.shape[2]\n",
    "\n",
    "only_dynamic_lstm_model = LSTMModel(num_time_steps, num_dynamic_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dynamic Input Tensor Shape: torch.Size([8326, 12, 835])\n",
      "Training Labels Tensor Shape: torch.Size([8326])\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Dynamic Input Tensor Shape:\", dynamic_train_tensor.shape)  # Expected: [332, 12, 834]\n",
    "print(\"Training Labels Tensor Shape:\", label_train_tensor.shape)  # Expected: [332]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch Dynamic Input Shape: torch.Size([16, 12, 835])\n",
      "Batch Labels Shape: torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "for dynamic_input, labels in dynamic_train_loader:\n",
    "    print(\"Batch Dynamic Input Shape:\", dynamic_input.shape)  # Should be something like [batch_size, 12, 834]\n",
    "    print(\"Batch Labels Shape:\", labels.shape)  # Should be [batch_size]\n",
    "    break  # Just to check the first batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply weights to the loss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Loss function and optimizer\n",
    "optimizer = optim.Adam(only_dynamic_lstm_model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "best_val_loss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Training Loss: 0.2769, Validation Loss: 0.2380\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dynamic_input, labels \u001b[38;5;129;01min\u001b[39;00m dynamic_train_loader:\n\u001b[1;32m----> 7\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m only_dynamic_lstm_model(dynamic_input)\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m# Use squeeze(1) to safely remove only the feature dimension\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\_compile.py:20\u001b[0m, in \u001b[0;36m_disable_dynamo.<locals>.inner\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03mThis API should be only used inside torch, external users should still use\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03mtorch._dynamo.disable. The main goal of this API is to avoid circular\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03mthe invocation of the decorated function.\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 20\u001b[0m     \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fn)\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dynamo\u001b[39;00m\n\u001b[0;32m     24\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mdisable(fn, recursive)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "early_stopping_patience = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    only_dynamic_lstm_model.train()\n",
    "    total_loss = 0\n",
    "    for dynamic_input, labels in dynamic_train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = only_dynamic_lstm_model(dynamic_input)\n",
    "        \n",
    "        # Use squeeze(1) to safely remove only the feature dimension\n",
    "        if outputs.dim() > 1 and outputs.shape[1] == 1:\n",
    "            outputs = outputs.squeeze(1)\n",
    "        \n",
    "        # Check shapes right before calculating the loss\n",
    "        if outputs.shape != labels.shape:\n",
    "            print(f\"Shape mismatch - Outputs: {outputs.shape}, Labels: {labels.shape}\")\n",
    "            outputs = outputs.view(-1)  # Ensure outputs are flat if still not flat\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Validation loss computation\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for dynamic_input, labels in dynamic_val_loader:\n",
    "            outputs = only_dynamic_lstm_model(dynamic_input).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    # Calculate average losses\n",
    "    avg_train_loss = total_loss / len(dynamic_train_loader)\n",
    "    avg_val_loss = val_loss / len(dynamic_val_loader)\n",
    "\n",
    "    # Print the average losses for the current epoch\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Check if the current validation loss is the best we've seen so far\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(only_dynamic_lstm_model.state_dict(), './saved_models/LSTM/state_dict/best_lstm_model_only_dynamic.pth')\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve == early_stopping_patience:\n",
    "            print(\"Early stopping initiated.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.2949295938014984\n",
      "Test Accuracy: 0.8591549396514893\n"
     ]
    }
   ],
   "source": [
    "# Load the best LSTM model\n",
    "only_dynamic_lstm_model.load_state_dict(torch.load('./saved_models/LSTM/state_dict/best_lstm_model_only_dynamic.pth'))\n",
    "\n",
    "# Make predictions\n",
    "only_dynamic_lstm_model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = only_dynamic_lstm_model(dynamic_test_tensor).squeeze()\n",
    "    test_loss = criterion(test_outputs, label_test_tensor)\n",
    "    test_predictions = (test_outputs > 0.5).int()\n",
    "\n",
    "print(f\"Test Loss: {test_loss.item()}\")\n",
    "print(f\"Test Accuracy: {(test_predictions == label_test_tensor.int()).float().mean().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_lstm_model(model, dynamic_test_tensor, label_test_tensor, directory, model_name, beta=2):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    sns.set()  # For better plot styling\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Predict on the test set\n",
    "        test_outputs = model(dynamic_test_tensor).squeeze()\n",
    "        test_probs = test_outputs.numpy()  # Probability predictions\n",
    "\n",
    "    # True labels for comparison\n",
    "    true_labels = label_test_tensor.numpy()\n",
    "\n",
    "    # Calculate precision-recall curve and corresponding thresholds\n",
    "    precision, recall, thresholds = precision_recall_curve(true_labels, test_probs)\n",
    "\n",
    "    # Calculate F-beta scores for each possible threshold\n",
    "    f_beta_scores = (1 + beta**2) * (precision * recall) / ((beta**2 * precision) + recall)\n",
    "    f_beta_scores = np.nan_to_num(f_beta_scores)  # Handling NaNs\n",
    "\n",
    "    # Find the threshold that maximizes the F-beta score\n",
    "    optimal_idx = np.argmax(f_beta_scores)\n",
    "    best_threshold = thresholds[optimal_idx]\n",
    "\n",
    "    # Use the best threshold found\n",
    "    test_predictions = (test_probs > best_threshold).astype(int)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(true_labels, test_predictions)\n",
    "    precision = precision_score(true_labels, test_predictions, zero_division=0)\n",
    "    recall = recall_score(true_labels, test_predictions)\n",
    "    f1 = f1_score(true_labels, test_predictions)\n",
    "    auc_roc = roc_auc_score(true_labels, test_probs)\n",
    "    auprc = average_precision_score(true_labels, test_probs)\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Best Threshold: {best_threshold:.2f}\")\n",
    "    print(f\"{model_name} Model Performance on Test Set:\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"Precision: {precision:.2f}\")\n",
    "    print(f\"Recall: {recall:.2f}\")\n",
    "    print(f\"F1 Score: {f1:.2f}\")\n",
    "    print(f\"AUC-ROC: {auc_roc:.2f}\")\n",
    "    print(f\"AUPRC: {auprc:.2f}\")\n",
    "\n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    # Save the confusion matrix\n",
    "    cm = confusion_matrix(true_labels, test_predictions)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{directory}/{model_name}_confusion_matrix.png')\n",
    "    plt.close()  # Close the plot to avoid display\n",
    "\n",
    "    # Save performance metrics to a text file\n",
    "    metrics_filepath = f'{directory}/{model_name}_metrics.txt'\n",
    "    with open(metrics_filepath, 'w') as f:\n",
    "        f.write(f\"{model_name} Model Performance on Test Set:\\n\")\n",
    "        f.write(f\"Best Threshold: {best_threshold:.2f}\\n\")\n",
    "        f.write(f\"Accuracy: {accuracy:.2f}\\n\")\n",
    "        f.write(f\"Precision: {precision:.2f}\\n\")\n",
    "        f.write(f\"Recall: {recall:.2f}\\n\")\n",
    "        f.write(f\"F1 Score: {f1:.2f}\\n\")\n",
    "        f.write(f\"AUC-ROC: {auc_roc:.2f}\\n\")\n",
    "        f.write(f\"AUPRC: {auprc:.2f}\\n\")\n",
    "\n",
    "    print(f\"Performance metrics saved to: {metrics_filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold: 0.08\n",
      "LSTM_Only_Dynamic_Features Model Performance on Test Set:\n",
      "Accuracy: 0.69\n",
      "Precision: 0.17\n",
      "Recall: 0.67\n",
      "F1 Score: 0.27\n",
      "AUC-ROC: 0.78\n",
      "AUPRC: 0.26\n",
      "Performance metrics saved to: ./saved_models/LSTM/lstm_model_only_dynamic_features_1%/LSTM_Only_Dynamic_Features_metrics.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soere\\AppData\\Local\\Temp\\ipykernel_18184\\2823572345.py:17: RuntimeWarning: invalid value encountered in divide\n",
      "  f_beta_scores = (1 + beta**2) * (precision * recall) / ((beta**2 * precision) + recall)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate LSTM model with only dynamic features\n",
    "evaluate_lstm_model(only_dynamic_lstm_model, dynamic_test_tensor, label_test_tensor, f'./saved_models/LSTM/lstm_model_only_dynamic_features_{percentage}', 'LSTM_Only_Dynamic_Features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the entire model\n",
    "torch.save(only_dynamic_lstm_model, f'./saved_models/LSTM/lstm_model_only_dynamic_features_{percentage}/lstm_model_only_dynamic_features_{percentage}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train LSTM Model Repeating Static Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset parameters\n",
    "best_val_loss = float('inf')\n",
    "epochs_no_improve = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soere\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\rnn.py:82: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "# Model instantiation\n",
    "num_time_steps = train_tensor_repeated_static_features.shape[1]\n",
    "num_dynamic_features = train_tensor_repeated_static_features.shape[2]\n",
    "num_static_features = train_tensor_repeated_static_features.shape[1]\n",
    "\n",
    "lstm_model_repeating_features = LSTMModel(num_time_steps, num_dynamic_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights\n",
    "class_counts = label_train_tensor.long().bincount()\n",
    "total_counts = class_counts.sum()\n",
    "class_weights = total_counts / class_counts  # This will give more weight to the minority class\n",
    "\n",
    "# Apply weights to the loss function\n",
    "weights = class_weights[label_train_tensor.long()]\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "#criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(lstm_model_repeating_features.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "best_val_loss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Training Loss: 0.5227, Validation Loss: 0.3939\n",
      "Epoch 2/100, Training Loss: 0.3063, Validation Loss: 0.3398\n",
      "Epoch 3/100, Training Loss: 0.2367, Validation Loss: 0.2845\n",
      "Epoch 4/100, Training Loss: 0.1890, Validation Loss: 0.2997\n",
      "Epoch 5/100, Training Loss: 0.1447, Validation Loss: 0.2888\n",
      "Epoch 6/100, Training Loss: 0.1025, Validation Loss: 0.2703\n",
      "Epoch 7/100, Training Loss: 0.0753, Validation Loss: 0.2663\n",
      "Epoch 8/100, Training Loss: 0.0466, Validation Loss: 0.2729\n",
      "Epoch 9/100, Training Loss: 0.0280, Validation Loss: 0.2856\n",
      "Epoch 10/100, Training Loss: 0.0228, Validation Loss: 0.3632\n",
      "Epoch 11/100, Training Loss: 0.0161, Validation Loss: 0.3256\n",
      "Epoch 12/100, Training Loss: 0.0118, Validation Loss: 0.3309\n",
      "Epoch 13/100, Training Loss: 0.0104, Validation Loss: 0.3556\n",
      "Epoch 14/100, Training Loss: 0.0084, Validation Loss: 0.3940\n",
      "Epoch 15/100, Training Loss: 0.0066, Validation Loss: 0.3327\n",
      "Epoch 16/100, Training Loss: 0.0056, Validation Loss: 0.3683\n",
      "Epoch 17/100, Training Loss: 0.0049, Validation Loss: 0.3802\n",
      "Early stopping initiated.\n"
     ]
    }
   ],
   "source": [
    "early_stopping_patience = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    lstm_model_repeating_features.train()\n",
    "    total_loss = 0\n",
    "    for dynamic_input, labels in repeated_static_features_train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = lstm_model_repeating_features(dynamic_input).squeeze()\n",
    "        outputs = outputs.view(-1)  # Flatten the output\n",
    "        labels = labels.view(-1)  # Flatten the labels\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Validation loss computation\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for dynamic_input, labels in repeated_static_features_val_loader:\n",
    "            outputs = lstm_model_repeating_features(dynamic_input).squeeze()\n",
    "            outputs = outputs.view(-1)\n",
    "            labels = labels.view(-1)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    # Calculate average losses\n",
    "    avg_train_loss = total_loss / len(repeated_static_features_train_loader)\n",
    "    avg_val_loss = val_loss / len(repeated_static_features_val_loader)\n",
    "\n",
    "    # Print the average losses for the current epoch\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Training Loss: {avg_train_loss:.4f}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # Check if the current validation loss is the best we've seen so far\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(lstm_model_repeating_features.state_dict(), './saved_models/LSTM/state_dict/best_lstm_model_repeating_features.pth')  # Save under a correct name\n",
    "        epochs_no_improve = 0\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve == early_stopping_patience:\n",
    "            print(\"Early stopping initiated.\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.2601236402988434\n",
      "Test Accuracy: 0.8873239159584045\n"
     ]
    }
   ],
   "source": [
    "# Load the best LSTM model\n",
    "lstm_model_repeating_features.load_state_dict(torch.load('./saved_models/LSTM/state_dict/best_lstm_model_repeating_features.pth'))\n",
    "\n",
    "# Make predictions\n",
    "lstm_model_repeating_features.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = lstm_model_repeating_features(test_tensor_repeated_static_features).squeeze()\n",
    "    test_loss = criterion(test_outputs, label_test_tensor)\n",
    "    test_predictions = (test_outputs > 0.5).int()\n",
    "\n",
    "print(f\"Test Loss: {test_loss.item()}\")\n",
    "print(f\"Test Accuracy: {(test_predictions == label_test_tensor.int()).float().mean().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold: 0.08\n",
      "LSTM_Repeating_Static_Features Model Performance on Test Set:\n",
      "Accuracy: 0.77\n",
      "Precision: 0.22\n",
      "Recall: 0.67\n",
      "F1 Score: 0.33\n",
      "AUC-ROC: 0.83\n",
      "AUPRC: 0.26\n",
      "Performance metrics saved to: ./saved_models/LSTM/lstm_model_repeating_static_features_1%/LSTM_Repeating_Static_Features_metrics.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soere\\AppData\\Local\\Temp\\ipykernel_18184\\2823572345.py:17: RuntimeWarning: invalid value encountered in divide\n",
      "  f_beta_scores = (1 + beta**2) * (precision * recall) / ((beta**2 * precision) + recall)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate LSTM model with only dynamic features\n",
    "evaluate_lstm_model(lstm_model_repeating_features, test_tensor_repeated_static_features, label_test_tensor, f'./saved_models/LSTM/lstm_model_repeating_static_features_{percentage}', 'LSTM_Repeating_Static_Features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the entire model\n",
    "torch.save(lstm_model_repeating_features, f'./saved_models/LSTM/lstm_model_repeating_static_features_{percentage}/lstm_model_repeating_static_features_{percentage}.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
