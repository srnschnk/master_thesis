{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import sys\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import pickle\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Custom Functions\n",
    "sys.path.append('./model')\n",
    "from custom_functions import load_raw_data, extract_icd_codes, extract_dynamic_data_dict, extract_demographic_features, summarize_dynamic_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files for 100% subset loaded successfully.\n",
      "Number of stays: 47581\n",
      "ICD Features shape: (47581, 1458)\n",
      "Dynamic Features shape: (570972, 835)\n",
      "Demographic Features shape: (47581, 4)\n"
     ]
    }
   ],
   "source": [
    "# Define the percentage to load\n",
    "percentage = '100%'  # Change this to '5%', '10%', etc., as needed\n",
    "\n",
    "# Base directory for the data subsets\n",
    "base_dir = f'./data/subsets/{percentage}_subsets/'\n",
    "\n",
    "# Load Labels\n",
    "labels = pd.read_csv(f'{base_dir}labels.csv')\n",
    "stay_ids = labels['stay_id'].unique()\n",
    "\n",
    "# Load static features\n",
    "icd_features = pd.read_pickle(f'{base_dir}icd_code_features.pkl')\n",
    "\n",
    "# Load summarized dynamic features\n",
    "dynamic_data_df = pd.read_pickle(f'{base_dir}dynamic_data_df.pkl')\n",
    "\n",
    "# Load demographic features\n",
    "demographic_features = pd.read_pickle(f'{base_dir}demographic_features.pkl')\n",
    "\n",
    "# Print information to confirm the files are loaded\n",
    "print(f\"Files for {percentage} subset loaded successfully.\")\n",
    "print(f\"Number of stays: {len(stay_ids)}\")\n",
    "print(f\"ICD Features shape: {icd_features.shape}\")\n",
    "print(f\"Dynamic Features shape: {dynamic_data_df.shape}\")\n",
    "print(f\"Demographic Features shape: {demographic_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenating the DataFrames along the columns (axis=1)\n",
    "all_static_features = pd.concat([icd_features, demographic_features], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split before any other operation to avoid Data Leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and temp sets (temp will be split into validation and test)\n",
    "train_stays, temp_stays = train_test_split(labels, test_size=0.3, random_state=42, stratify=labels['label'])\n",
    "\n",
    "# Now split temp into test and validation sets equally\n",
    "test_stays, val_stays = train_test_split(temp_stays, test_size=0.5, random_state=42, stratify=temp_stays['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Order by stay_id\n",
    "train_stays = train_stays.sort_values(\"stay_id\")\n",
    "test_stays = test_stays.sort_values(\"stay_id\")\n",
    "val_stays = val_stays.sort_values(\"stay_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Train Tensor shape: torch.Size([33306])\n",
      "Label Test Tensor shape: torch.Size([7137])\n",
      "Label Validation Tensor shape: torch.Size([7138])\n",
      "Fraction of labels that are 1: 0.08280790248003363\n",
      "Fraction of labels that are 1: 0.08280790248003363\n",
      "Fraction of labels that are 1: 0.08279630148500981\n"
     ]
    }
   ],
   "source": [
    "# Convert label columns directly to tensors\n",
    "label_tensor_train = torch.tensor(train_stays['label'].values, dtype=torch.float32)\n",
    "label_tensor_test = torch.tensor(test_stays['label'].values, dtype=torch.float32)\n",
    "label_tensor_val = torch.tensor(val_stays['label'].values, dtype=torch.float32)\n",
    "\n",
    "# Print shapes to confirm\n",
    "print(\"Label Train Tensor shape:\", label_tensor_train.shape)\n",
    "print(\"Label Test Tensor shape:\", label_tensor_test.shape)\n",
    "print(\"Label Validation Tensor shape:\", label_tensor_val.shape)\n",
    "\n",
    "# Calculate the fraction of positive labels (label=1)\n",
    "fraction_positive_train = train_stays['label'].mean()\n",
    "fraction_positive_test = test_stays['label'].mean()\n",
    "fraction_positive_val = val_stays['label'].mean()\n",
    "\n",
    "print(\"Fraction of labels that are 1:\", fraction_positive_train)\n",
    "print(\"Fraction of labels that are 1:\", fraction_positive_test)\n",
    "print(\"Fraction of labels that are 1:\", fraction_positive_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling & Encoding of Static Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Slices of all_static features for each set\n",
    "static_features_train = all_static_features.loc[train_stays['stay_id']]\n",
    "static_features_val = all_static_features.loc[val_stays['stay_id']]\n",
    "static_features_test = all_static_features.loc[test_stays['stay_id']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continous Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting Continious Columns for the Different Sets\n",
    "continous_static_columns_train = static_features_train[[\"Age\"]]\n",
    "continous_static_columns_val = static_features_val[[\"Age\"]]\n",
    "continous_static_columns_test = static_features_test[[\"Age\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Standard Scaler on the Train Set and apply it to Train, Validation and Test Set \n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data\n",
    "scaler.fit(continous_static_columns_train)\n",
    "\n",
    "# Scale Training Set in a way that yields a Data Frame again\n",
    "continous_static_columns_train = pd.DataFrame(\n",
    "    scaler.transform(continous_static_columns_train),\n",
    "    index=continous_static_columns_train.index,\n",
    "    columns=continous_static_columns_train.columns\n",
    ")\n",
    "\n",
    "# Scale Training Set in a way that yields a Data Frame again\n",
    "continous_static_columns_val = pd.DataFrame(\n",
    "    scaler.transform(continous_static_columns_val),\n",
    "    index=continous_static_columns_val.index,\n",
    "    columns=continous_static_columns_val.columns\n",
    ")\n",
    "\n",
    "# Scale Training Set in a way that yields a Data Frame again\n",
    "continous_static_columns_test = pd.DataFrame(\n",
    "    scaler.transform(continous_static_columns_test),\n",
    "    index=continous_static_columns_test.index,\n",
    "    columns=continous_static_columns_test.columns\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting Categorical Columns for the Different Sets\n",
    "categorical_columns = ['gender', 'ethnicity', 'insurance']  \n",
    "categorical_static_columns_train = static_features_train[categorical_columns]\n",
    "categorical_static_columns_val = static_features_val[categorical_columns]\n",
    "categorical_static_columns_test = static_features_test[categorical_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\soere\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\preprocessing\\_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize the encoder\n",
    "encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "\n",
    "# Fit the encoder on the training data\n",
    "encoder.fit(categorical_static_columns_train)\n",
    "\n",
    "# Apply the encoder to the train, validation, and test data\n",
    "categorical_static_columns_train_encoded = pd.DataFrame(\n",
    "    encoder.transform(categorical_static_columns_train),\n",
    "    index=categorical_static_columns_train.index,\n",
    "    columns=encoder.get_feature_names_out(categorical_columns)\n",
    ")\n",
    "\n",
    "categorical_static_columns_val_encoded = pd.DataFrame(\n",
    "    encoder.transform(categorical_static_columns_val),\n",
    "    index=categorical_static_columns_val.index,\n",
    "    columns=encoder.get_feature_names_out(categorical_columns)\n",
    ")\n",
    "\n",
    "categorical_static_columns_test_encoded = pd.DataFrame(\n",
    "    encoder.transform(categorical_static_columns_test),\n",
    "    index=categorical_static_columns_test.index,\n",
    "    columns=encoder.get_feature_names_out(categorical_columns)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boolean Features (ICD Codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select Boolean columns for each set (ICD Codes)\n",
    "bool_static_columns_train = static_features_train[icd_features.columns]\n",
    "bool_static_columns_val = static_features_val[icd_features.columns]\n",
    "bool_static_columns_test = static_features_test[icd_features.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Boolean Columns to Float for Consistent Formatting\n",
    "bool_static_columns_train = bool_static_columns_train.astype(float)\n",
    "bool_static_columns_val = bool_static_columns_val.astype(float)\n",
    "bool_static_columns_test = bool_static_columns_test.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine Data Frames Back into one per Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all parts into one DataFrame using pd.concat\n",
    "processed_static_features_train = pd.concat([continous_static_columns_train, categorical_static_columns_train_encoded, bool_static_columns_train], axis=1)\n",
    "processed_static_features_test = pd.concat([continous_static_columns_test, categorical_static_columns_test_encoded, bool_static_columns_test], axis=1)\n",
    "processed_static_features_val = pd.concat([continous_static_columns_val, categorical_static_columns_val_encoded, bool_static_columns_val], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure sorting by stay_id\n",
    "processed_static_features_train = processed_static_features_train.sort_index()\n",
    "processed_static_features_test = processed_static_features_test.sort_index()\n",
    "processed_static_features_val = processed_static_features_val.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reordered Static Train Tensor shape: torch.Size([33306, 1497])\n",
      "Reordered Static Test Tensor shape: torch.Size([7137, 1497])\n",
      "Reordered Static Validation Tensor shape: torch.Size([7138, 1497])\n"
     ]
    }
   ],
   "source": [
    "# Convert reindexed static data to tensors\n",
    "static_train_tensor = torch.tensor(processed_static_features_train.values, dtype=torch.float32)\n",
    "static_test_tensor = torch.tensor(processed_static_features_test.values, dtype=torch.float32)\n",
    "static_val_tensor = torch.tensor(processed_static_features_val.values, dtype=torch.float32)\n",
    "\n",
    "# Print the shapes to confirm the reordering\n",
    "print(\"Reordered Static Train Tensor shape:\", static_train_tensor.shape)\n",
    "print(\"Reordered Static Test Tensor shape:\", static_test_tensor.shape)\n",
    "print(\"Reordered Static Validation Tensor shape:\", static_val_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Dynamic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the multi-level columns into a single level\n",
    "flattened_columns = ['_'.join(map(str, col)) for col in dynamic_data_df.columns]\n",
    "\n",
    "# Update the DataFrame with flattened columns\n",
    "dynamic_data_df.columns = flattened_columns\n",
    "\n",
    "# Rename 'stay_id_' column to 'stay_id'\n",
    "dynamic_data_df = dynamic_data_df.rename(columns={'stay_id_': 'stay_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep timestep as column to ensure right temporal order\n",
    "dynamic_data_df[\"timestep\"] = dynamic_data_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Subsets of Dynamic Data Dataframe for Test Train and Validation Set\n",
    "dynamic_train_data = dynamic_data_df[dynamic_data_df[\"stay_id\"].isin(train_stays[\"stay_id\"])]\n",
    "dynamic_test_data = dynamic_data_df[dynamic_data_df[\"stay_id\"].isin(test_stays[\"stay_id\"])]\n",
    "dynamic_val_data = dynamic_data_df[dynamic_data_df[\"stay_id\"].isin(val_stays[\"stay_id\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a scaler for dynamic features\n",
    "dynamic_scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the stay_id before scaling\n",
    "train_stay_ids = dynamic_train_data.pop('stay_id')\n",
    "test_stay_ids = dynamic_test_data.pop('stay_id')\n",
    "val_stay_ids = dynamic_val_data.pop('stay_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the dynamic features\n",
    "dynamic_train_norm_array = dynamic_scaler.fit_transform(dynamic_train_data)\n",
    "dynamic_test_norm_array = dynamic_scaler.transform(dynamic_test_data)\n",
    "dynamic_val_norm_array = dynamic_scaler.transform(dynamic_val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the resulting arrays back to a DataFrame\n",
    "dynamic_train_data_norm = pd.DataFrame(\n",
    "    dynamic_train_norm_array, \n",
    "    index=dynamic_train_data.index, \n",
    "    columns=dynamic_train_data.columns\n",
    ")\n",
    "dynamic_train_data_norm['stay_id'] = train_stay_ids.values\n",
    "\n",
    "dynamic_test_data_norm = pd.DataFrame(\n",
    "    dynamic_test_norm_array, \n",
    "    index=dynamic_test_data.index, \n",
    "    columns=dynamic_test_data.columns\n",
    ")\n",
    "dynamic_test_data_norm['stay_id'] = test_stay_ids.values\n",
    "\n",
    "dynamic_val_data_norm = pd.DataFrame(\n",
    "    dynamic_val_norm_array, \n",
    "    index=dynamic_val_data.index, \n",
    "    columns=dynamic_val_data.columns\n",
    ")\n",
    "dynamic_val_data_norm['stay_id'] = val_stay_ids.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add timestep as column to ensure temporal order\n",
    "dynamic_train_data_norm[\"timestep\"] = dynamic_train_data_norm.index\n",
    "dynamic_test_data_norm[\"timestep\"] = dynamic_test_data_norm.index\n",
    "dynamic_val_data_norm[\"timestep\"] = dynamic_val_data_norm.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort data by stay_id and timestep\n",
    "dynamic_train_data_norm = (dynamic_train_data_norm\n",
    "                                  .sort_values('stay_id')\n",
    "                                  .groupby('stay_id')\n",
    "                                  .apply(lambda x: x.sort_index())\n",
    "                                  .reset_index(drop=True))\n",
    "\n",
    "dynamic_test_data_norm = (dynamic_test_data_norm\n",
    "                                  .sort_values('stay_id')\n",
    "                                  .groupby('stay_id')\n",
    "                                  .apply(lambda x: x.sort_index())\n",
    "                                  .reset_index(drop=True))\n",
    "\n",
    "dynamic_val_data_norm = (dynamic_val_data_norm\n",
    "                                  .sort_values('stay_id')\n",
    "                                  .groupby('stay_id')\n",
    "                                  .apply(lambda x: x.sort_index())\n",
    "                                  .reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure dynamic data is ordered by stay_id and timestep\n",
    "dynamic_train_data_norm = dynamic_train_data_norm.sort_values([\"stay_id\", \"timestep\"])\n",
    "dynamic_test_data_norm = dynamic_test_data_norm.sort_values([\"stay_id\", \"timestep\"])\n",
    "dynamic_val_data_norm = dynamic_val_data_norm.sort_values([\"stay_id\", \"timestep\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Dynamic Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy dataframes before conversion to tensor to keep the original for further processing\n",
    "dynamic_train_data_norm_copy = dynamic_train_data_norm.copy()\n",
    "dynamic_test_data_norm_copy = dynamic_test_data_norm.copy()\n",
    "dynamic_val_data_norm_copy = dynamic_val_data_norm.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop stay_id and timestep before conversion to tensor\n",
    "dynamic_train_data_norm_copy.drop(columns=[\"stay_id\", \"timestep\"], inplace=True)\n",
    "dynamic_test_data_norm_copy.drop(columns=[\"stay_id\", \"timestep\"], inplace=True)\n",
    "dynamic_val_data_norm_copy.drop(columns=[\"stay_id\", \"timestep\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_time_steps = 12\n",
    "\n",
    "def reshape_dynamic_data(df, num_time_steps):\n",
    "    # This will ensure all stays have the required number of time steps\n",
    "    output_array = []\n",
    "    stay_id_order = []\n",
    "    for stay_id, group in df.groupby('stay_id'):\n",
    "        if len(group) == num_time_steps:\n",
    "            output_array.append(group.drop(columns=['stay_id']).values)\n",
    "            stay_id_order.append(stay_id)\n",
    "        else:\n",
    "            print(f\"Stay ID {stay_id} has {len(group)} time steps, expected {num_time_steps}\")\n",
    "    return np.array(output_array), stay_id_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dynamic Train Tensor shape: torch.Size([33306, 12, 835])\n",
      "Dynamic Test Tensor shape: torch.Size([7137, 12, 835])\n",
      "Dynamic Validation Tensor shape: torch.Size([7138, 12, 835])\n"
     ]
    }
   ],
   "source": [
    "# Reshape the data and capture stay_id order\n",
    "dynamic_features_train_array, train_stay_order = reshape_dynamic_data(dynamic_train_data_norm, num_time_steps)\n",
    "dynamic_features_test_array, test_stay_order = reshape_dynamic_data(dynamic_test_data_norm, num_time_steps)\n",
    "dynamic_features_val_array, val_stay_order = reshape_dynamic_data(dynamic_val_data_norm, num_time_steps)\n",
    "\n",
    "# Convert numpy arrays to tensors\n",
    "dynamic_train_tensor = torch.tensor(dynamic_features_train_array, dtype=torch.float32)\n",
    "dynamic_test_tensor = torch.tensor(dynamic_features_test_array, dtype=torch.float32)\n",
    "dynamic_val_tensor = torch.tensor(dynamic_features_val_array, dtype=torch.float32)\n",
    "\n",
    "# Print shapes to confirm\n",
    "print(\"Dynamic Train Tensor shape:\", dynamic_train_tensor.shape)  # Expected: [num_stays, 12, num_features]\n",
    "print(\"Dynamic Test Tensor shape:\", dynamic_test_tensor.shape)\n",
    "print(\"Dynamic Validation Tensor shape:\", dynamic_val_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to save tensors\n",
    "tensor_save_path = f'./data/tensors/{percentage}_subset'\n",
    "\n",
    "# Create directory if it doesn't exist\n",
    "if not os.path.exists(tensor_save_path):\n",
    "    os.makedirs(tensor_save_path)\n",
    "\n",
    "# Save tensors\n",
    "torch.save(dynamic_train_tensor, os.path.join(tensor_save_path, 'dynamic_train_tensor.pt'))\n",
    "torch.save(static_train_tensor, os.path.join(tensor_save_path, 'static_train_tensor.pt'))\n",
    "torch.save(label_tensor_train, os.path.join(tensor_save_path, 'label_train_tensor.pt'))\n",
    "\n",
    "torch.save(dynamic_test_tensor, os.path.join(tensor_save_path, 'dynamic_test_tensor.pt'))\n",
    "torch.save(static_test_tensor, os.path.join(tensor_save_path, 'static_test_tensor.pt'))\n",
    "torch.save(label_tensor_test, os.path.join(tensor_save_path, 'label_test_tensor.pt'))\n",
    "\n",
    "torch.save(dynamic_val_tensor, os.path.join(tensor_save_path, 'dynamic_val_tensor.pt'))\n",
    "torch.save(static_val_tensor, os.path.join(tensor_save_path, 'static_val_tensor.pt'))\n",
    "torch.save(label_tensor_val, os.path.join(tensor_save_path, 'label_val_tensor.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Oversampled Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oversample Train Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the classes\n",
    "majority = train_stays[train_stays['label'] == 0]\n",
    "minority = train_stays[train_stays['label'] == 1]\n",
    "\n",
    "# Upsample minority class\n",
    "minority_upsampled = resample(minority,\n",
    "                              replace=True,            # sample with replacement\n",
    "                              n_samples=len(majority), # to match majority class\n",
    "                              random_state=123)        # reproducible results\n",
    "\n",
    "# Combine majority class with upsampled minority class\n",
    "oversampled_train_labels = pd.concat([majority, minority_upsampled])\n",
    "\n",
    "# Add a new column to uniquely identify each row even after upsampling\n",
    "oversampled_train_labels['stay_id_replica'] = oversampled_train_labels.groupby('stay_id').cumcount()\n",
    "\n",
    "# Create a unique identifier combining the original stay_id with the replica indicator\n",
    "oversampled_train_labels['unique_stay_id'] = oversampled_train_labels['stay_id'].astype(str) + \"_\" + oversampled_train_labels['stay_id_replica'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampled Label Tensor shape: torch.Size([61096])\n"
     ]
    }
   ],
   "source": [
    "# Sort the oversampled labels by stay_id and replica number\n",
    "oversampled_train_labels = oversampled_train_labels.sort_values(by=[\"stay_id\", \"stay_id_replica\"])\n",
    "\n",
    "# Convert label columns directly to tensors\n",
    "label_tensor_train_oversampled = torch.tensor(oversampled_train_labels['label'].values, dtype=torch.float32)\n",
    "\n",
    "print(\"Oversampled Label Tensor shape:\", label_tensor_train_oversampled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oversample Dynamic Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Count stay_ids repetitions\n",
    "stay_id_counts = oversampled_train_labels['stay_id'].value_counts()\n",
    "\n",
    "# Add the count to dynamic_train_data\n",
    "dynamic_train_data_norm_with_count = dynamic_train_data_norm.merge(stay_id_counts, on='stay_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total number of rows for the expanded dataframe\n",
    "total_rows = dynamic_train_data_norm_with_count['count'].sum()\n",
    "\n",
    "# Prepare an empty list to hold the expanded data\n",
    "expanded_data = []\n",
    "\n",
    "# Prepare an empty DataFrame with the same columns plus the unique_stay_id\n",
    "columns_with_id = dynamic_train_data_norm_with_count.columns.tolist() + ['replica']\n",
    "oversampled_dynamic_train_data_norm = pd.DataFrame(columns=columns_with_id)\n",
    "\n",
    "# Use iterrows to duplicate each row and add a unique identifier\n",
    "all_rows = []  # List to collect all new rows\n",
    "\n",
    "for index, row in dynamic_train_data_norm_with_count.iterrows():\n",
    "    stay_id = row['stay_id']  # Get the stay_id from the row\n",
    "    for i in range(int(row['count'])):  # Duplicate the row 'count' times\n",
    "        new_row = row.copy()  # Copy the row to modify it without affecting the original DataFrame\n",
    "        new_row['replica'] = i  # Create a unique identifier for each duplicated row\n",
    "        all_rows.append(new_row)  # Append the modified row to the list\n",
    "\n",
    "# Convert the list of rows back into a DataFrame\n",
    "oversampled_dynamic_train_data_norm = pd.DataFrame(all_rows, columns=columns_with_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort Dataframe by stay_id, replica and timestep\n",
    "oversampled_dynamic_train_data_norm = oversampled_dynamic_train_data_norm.sort_values([\"stay_id\", \"replica\", \"timestep\"])\n",
    "\n",
    "# Create unique_stay_id column\n",
    "oversampled_dynamic_train_data_norm[\"unique_stay_id\"] = oversampled_dynamic_train_data_norm['stay_id'].astype(str) + \"_\" + oversampled_dynamic_train_data_norm[\"replica\"].astype(str)\n",
    "\n",
    "# Drop meaningless columns before coversion to tensor\n",
    "oversampled_dynamic_train_data_norm = oversampled_dynamic_train_data_norm.drop(columns=[\"stay_id\", \"replica\", \"timestep\", \"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Oversampled Dynamic Tensor\n",
    "def reshape_oversampled_dynamic_data(df, num_time_steps):\n",
    "    # This will ensure all stays have the required number of time steps\n",
    "    output_array = []\n",
    "    stay_id_order = []\n",
    "    for unique_stay_id, group in df.groupby('unique_stay_id'):\n",
    "        if len(group) == num_time_steps:\n",
    "            # Drop both 'unique_stay_id' and 'stay_id' if present to clean up non-predictive identifiers\n",
    "            clean_group = group.drop(columns=['unique_stay_id'], errors='ignore')\n",
    "            output_array.append(clean_group.values)\n",
    "            stay_id_order.append(unique_stay_id)  # Keep tracking 'stay_id' for mapping predictions back to entities\n",
    "        else:\n",
    "            print(f\"Stay ID {stay_id} has {len(group)} time steps, expected {num_time_steps}\")\n",
    "    return np.array(output_array), stay_id_order\n",
    "# Reshape the data and capture stay_id order\n",
    "\n",
    "dynamic_features_train_array_oversampled, train_stay_order = reshape_oversampled_dynamic_data(oversampled_dynamic_train_data_norm, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oversampled Dynamic Train Tensor shape: torch.Size([30548, 12, 834])\n"
     ]
    }
   ],
   "source": [
    "# Convert numpy arrays to tensors\n",
    "dynamic_train_tensor_oversampled = torch.tensor(dynamic_features_train_array_oversampled, dtype=torch.float32)\n",
    "\n",
    "print(\"Oversampled Dynamic Train Tensor shape:\", dynamic_train_tensor_oversampled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oversample Static Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make stay_id string\n",
    "oversampled_train_labels[\"stay_id\"] = oversampled_train_labels[\"stay_id\"].astype(str)\n",
    "\n",
    "# Reset index of static_features_train to make 'stay_id' a column\n",
    "processed_static_features_train.reset_index(inplace=True)\n",
    "processed_static_features_train.rename(columns={'index': 'stay_id'}, inplace=True)\n",
    "\n",
    "# Convert stay_id to string\n",
    "processed_static_features_train[\"stay_id\"] = processed_static_features_train[\"stay_id\"].astype(str)\n",
    "stay_id_counts.index = stay_id_counts.index.astype(str)\n",
    "\n",
    "# Add counts and unique stay_id to features\n",
    "proc_static_features_train_with_counts = processed_static_features_train.merge(stay_id_counts, left_on='stay_id', right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand the DataFrame while tracking the repetition index\n",
    "oversampled_list = []\n",
    "for idx, row in proc_static_features_train_with_counts.iterrows():\n",
    "    repeat_count = int(row['count'])\n",
    "    for n in range(repeat_count):\n",
    "        # Create a new row as a copy of the current one\n",
    "        new_row = row.copy()\n",
    "        # Generate a unique identifier for each repeated instance\n",
    "        new_row['replica'] = n\n",
    "        oversampled_list.append(new_row)\n",
    "\n",
    "# Convert the list of Series objects into a DataFrame\n",
    "proc_static_features_train_oversampled = pd.DataFrame(oversampled_list)\n",
    "\n",
    "# Reset index as the append operation in the loop above will generate an index based on the original DataFrame\n",
    "proc_static_features_train_oversampled.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort both dataframes by stay_id and the replica number\n",
    "proc_static_features_train_oversampled.sort_values(by=[\"stay_id\", \"replica\"], inplace=True)\n",
    "oversampled_train_labels.sort_values(by=[\"stay_id\", \"stay_id_replica\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do the unique_stay_id columns match across both DataFrames? True\n"
     ]
    }
   ],
   "source": [
    "## Check that the unique_stay_ids are identical \n",
    "# Extract unique_stay_id columns\n",
    "proc_static_features_train_oversampled[\"unique_stay_id\"] = proc_static_features_train_oversampled[\"stay_id\"].astype(str) + \"_\" + proc_static_features_train_oversampled[\"replica\"].astype(str)\n",
    "proc_unique_ids = proc_static_features_train_oversampled['unique_stay_id']\n",
    "labels_unique_ids = oversampled_train_labels['unique_stay_id']\n",
    "\n",
    "# Reset indices before comparison to avoid issues with misaligned indices\n",
    "proc_unique_ids_reset = proc_unique_ids.reset_index(drop=True)\n",
    "labels_unique_ids_reset = labels_unique_ids.reset_index(drop=True)\n",
    "\n",
    "# Check if both columns are identical\n",
    "ids_match = proc_unique_ids_reset.equals(labels_unique_ids_reset)\n",
    "\n",
    "# Print the result\n",
    "print(\"Do the unique_stay_id columns match across both DataFrames?\", ids_match)\n",
    "\n",
    "# Find differences (if any)\n",
    "if not ids_match:\n",
    "    diff = proc_unique_ids_reset[proc_unique_ids_reset != labels_unique_ids_reset]\n",
    "    print(\"Differences found at indices:\", diff.index.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reordered Static Train Tensor shape: torch.Size([30548, 1497])\n"
     ]
    }
   ],
   "source": [
    "# Drop meaningless columns before creating tensors\n",
    "proc_static_features_train_oversampled.drop(columns=[\"unique_stay_id\", \"count\", \"replica\", \"stay_id\"], inplace=True)\n",
    "\n",
    "# Convert reindexed static data to tensors\n",
    "static_train_tensor_oversampled = torch.tensor(proc_static_features_train_oversampled.values, dtype=torch.float32)\n",
    "# Print the shapes to confirm the reordering\n",
    "print(\"Reordered Static Train Tensor shape:\", static_train_tensor_oversampled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversampled Train Tensors\n",
    "torch.save(label_tensor_train_oversampled, os.path.join(tensor_save_path, 'label_tensor_train_oversampled.pt'))\n",
    "torch.save(static_train_tensor_oversampled, os.path.join(tensor_save_path, 'static_train_tensor_oversampled.pt'))\n",
    "torch.save(dynamic_train_tensor_oversampled, os.path.join(tensor_save_path, 'dynamic_train_tensor_oversampled.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete Oversampled Objects to free up memory\n",
    "del proc_static_features_train_oversampled, oversampled_dynamic_train_data_norm, dynamic_features_train_array_oversampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Data with repeated static features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make stay_id a column for all sets\n",
    "# already a column for train set\n",
    "processed_static_features_test.reset_index(inplace=True)\n",
    "processed_static_features_val.reset_index(inplace=True)\n",
    "\n",
    "processed_static_features_test.rename(columns={'index': 'stay_id'}, inplace=True)\n",
    "processed_static_features_val.rename(columns={'index': 'stay_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat each row 12 times\n",
    "repeats = 12\n",
    "\n",
    "repeated_static_train_features = processed_static_features_train.loc[processed_static_features_train.index.repeat(repeats)].reset_index(drop=True)\n",
    "repeated_static_test_features = processed_static_features_test.loc[processed_static_features_test.index.repeat(repeats)].reset_index(drop=True)\n",
    "repeated_static_val_features = processed_static_features_val.loc[processed_static_features_val.index.repeat(repeats)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'stay_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\indexes\\base.py:3652\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3651\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3652\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3653\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\_libs\\index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\_libs\\index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'stay_id'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Create a running number for each repeat of 12 and concatenate with stay_id\u001b[39;00m\n\u001b[0;32m      2\u001b[0m repeated_static_train_features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msequence\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtile(np\u001b[38;5;241m.\u001b[39marange(repeats), \u001b[38;5;28mlen\u001b[39m(processed_static_features_train))\n\u001b[1;32m----> 3\u001b[0m repeated_static_train_features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstay_id_x\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mrepeated_static_train_features\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstay_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m repeated_static_train_features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msequence\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m      5\u001b[0m repeated_static_test_features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msequence\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtile(np\u001b[38;5;241m.\u001b[39marange(repeats), \u001b[38;5;28mlen\u001b[39m(processed_static_features_test))\n\u001b[0;32m      6\u001b[0m repeated_static_test_features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstay_id_x\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m repeated_static_test_features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstay_id\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m repeated_static_test_features[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msequence\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mstr\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\indexes\\base.py:3654\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3652\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3653\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3654\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3655\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3656\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3657\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3658\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3659\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'stay_id'"
     ]
    }
   ],
   "source": [
    "# Create a running number for each repeat of 12 and concatenate with stay_id\n",
    "repeated_static_train_features['sequence'] = np.tile(np.arange(repeats), len(processed_static_features_train))\n",
    "repeated_static_train_features['stay_id_x'] = repeated_static_train_features['stay_id'].astype(str) + \"_\" + repeated_static_train_features['sequence'].astype(str)\n",
    "\n",
    "repeated_static_test_features['sequence'] = np.tile(np.arange(repeats), len(processed_static_features_test))\n",
    "repeated_static_test_features['stay_id_x'] = repeated_static_test_features['stay_id'].astype(str) + \"_\" + repeated_static_test_features['sequence'].astype(str)\n",
    "\n",
    "repeated_static_val_features['sequence'] = np.tile(np.arange(repeats), len(processed_static_features_val))\n",
    "repeated_static_val_features['stay_id_x'] = repeated_static_val_features['stay_id'].astype(str) + \"_\" + repeated_static_val_features['sequence'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the sequence column if it's no longer needed\n",
    "repeated_static_train_features.drop(columns='sequence', inplace=True)\n",
    "repeated_static_test_features.drop(columns='sequence', inplace=True)\n",
    "repeated_static_val_features.drop(columns='sequence', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create stay_id_time_step index in Dynamic Data for merging with repeated static data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'stay_id_x' by combining 'stay_id' and 'time_step'\n",
    "dynamic_train_data_norm['stay_id_x'] = dynamic_train_data_norm['stay_id'].astype(str) + \"_\" + dynamic_train_data_norm['timestep'].astype(str)\n",
    "dynamic_test_data_norm['stay_id_x'] = dynamic_test_data_norm['stay_id'].astype(str) + \"_\" + dynamic_test_data_norm['timestep'].astype(str)\n",
    "dynamic_val_data_norm['stay_id_x'] = dynamic_val_data_norm['stay_id'].astype(str) + \"_\" + dynamic_val_data_norm['timestep'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dynamic_train_data_norm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[71], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Convert all columns to float32 except 'stay_id' and 'stay_id_x'\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m cols \u001b[38;5;241m=\u001b[39m \u001b[43mdynamic_train_data_norm\u001b[49m\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat64\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mdifference([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstay_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstay_id_x\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      3\u001b[0m dynamic_train_data_norm[cols] \u001b[38;5;241m=\u001b[39m dynamic_train_data_norm[cols]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dynamic_train_data_norm' is not defined"
     ]
    }
   ],
   "source": [
    "# Convert all columns to float32 except 'stay_id' and 'stay_id_x'\n",
    "cols = dynamic_train_data_norm.select_dtypes(include=['float64', 'int']).columns.difference(['stay_id', 'stay_id_x'])\n",
    "dynamic_train_data_norm[cols] = dynamic_train_data_norm[cols].astype('float32')\n",
    "dynamic_test_data_norm[cols] = dynamic_test_data_norm[cols].astype('float32')\n",
    "dynamic_val_data_norm[cols] = dynamic_val_data_norm[cols].astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge dynamic and static data on stay_id_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the merge\n",
    "all_train_features = pd.merge(\n",
    "    dynamic_train_data_norm, \n",
    "    repeated_static_train_features, \n",
    "    on='stay_id_x', \n",
    "    how='inner', \n",
    "    suffixes=('_dynamic', '_static')  # Suffixes to resolve any other column name conflicts\n",
    ")\n",
    "\n",
    "all_test_features = pd.merge(\n",
    "    dynamic_test_data_norm, \n",
    "    repeated_static_test_features, \n",
    "    on='stay_id_x', \n",
    "    how='inner', \n",
    "    suffixes=('_dynamic', '_static')  # Suffixes to resolve any other column name conflicts\n",
    ")\n",
    "\n",
    "all_val_features = pd.merge(\n",
    "    dynamic_val_data_norm, \n",
    "    repeated_static_val_features, \n",
    "    on='stay_id_x', \n",
    "    how='inner', \n",
    "    suffixes=('_dynamic', '_static')  # Suffixes to resolve any other column name conflicts\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 2.23 GiB for an array with shape (1497, 199836) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[69], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Make sure all Dataframes are sorted by stay_id and timestep\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m all_train_features \u001b[38;5;241m=\u001b[39m \u001b[43mall_train_features\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstay_id_static\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtimestep\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m all_test_features \u001b[38;5;241m=\u001b[39m all_test_features\u001b[38;5;241m.\u001b[39msort_values([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstay_id_static\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestep\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      4\u001b[0m all_val_features \u001b[38;5;241m=\u001b[39m all_val_features\u001b[38;5;241m.\u001b[39msort_values([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstay_id_static\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestep\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\frame.py:6789\u001b[0m, in \u001b[0;36mDataFrame.sort_values\u001b[1;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[0;32m   6786\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m   6788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_range_indexer(indexer, \u001b[38;5;28mlen\u001b[39m(indexer)):\n\u001b[1;32m-> 6789\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6790\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ignore_index:\n\u001b[0;32m   6791\u001b[0m         result\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m default_index(\u001b[38;5;28mlen\u001b[39m(result))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\generic.py:6452\u001b[0m, in \u001b[0;36mNDFrame.copy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m   6342\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[0;32m   6343\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcopy\u001b[39m(\u001b[38;5;28mself\u001b[39m: NDFrameT, deep: bool_t \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NDFrameT:\n\u001b[0;32m   6344\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   6345\u001b[0m \u001b[38;5;124;03m    Make a copy of this object's indices and data.\u001b[39;00m\n\u001b[0;32m   6346\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   6450\u001b[0m \u001b[38;5;124;03m    dtype: object\u001b[39;00m\n\u001b[0;32m   6451\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 6452\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6453\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clear_item_cache()\n\u001b[0;32m   6454\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor(data)\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcopy\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\internals\\managers.py:653\u001b[0m, in \u001b[0;36mBaseBlockManager.copy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    650\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    651\u001b[0m     new_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n\u001b[1;32m--> 653\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcopy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdeep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    654\u001b[0m res\u001b[38;5;241m.\u001b[39maxes \u001b[38;5;241m=\u001b[39m new_axes\n\u001b[0;32m    656\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    657\u001b[0m     \u001b[38;5;66;03m# Avoid needing to re-compute these\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\internals\\managers.py:352\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[1;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[0;32m    350\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    351\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 352\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    353\u001b[0m     result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[0;32m    355\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\pandas\\core\\internals\\blocks.py:540\u001b[0m, in \u001b[0;36mBlock.copy\u001b[1;34m(self, deep)\u001b[0m\n\u001b[0;32m    538\u001b[0m refs: BlockValuesRefs \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    539\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[1;32m--> 540\u001b[0m     values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    541\u001b[0m     refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 2.23 GiB for an array with shape (1497, 199836) and data type float64"
     ]
    }
   ],
   "source": [
    "# Make sure all Dataframes are sorted by stay_id and timestep\n",
    "all_train_features = all_train_features.sort_values([\"stay_id_static\", \"timestep\"])\n",
    "all_test_features = all_test_features.sort_values([\"stay_id_static\", \"timestep\"])\n",
    "all_val_features = all_val_features.sort_values([\"stay_id_static\", \"timestep\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop redundant stay_id columns\n",
    "all_train_features.drop(columns='stay_id_dynamic', inplace=True)\n",
    "all_train_features.drop(columns='stay_id_x', inplace=True)\n",
    "\n",
    "all_test_features.drop(columns='stay_id_dynamic', inplace=True)\n",
    "all_test_features.drop(columns='stay_id_x', inplace=True)\n",
    "\n",
    "all_val_features.drop(columns='stay_id_dynamic', inplace=True)\n",
    "all_val_features.drop(columns='stay_id_x', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename stay_id_static back to stay_id\n",
    "all_train_features.rename(columns={'stay_id_static': 'stay_id'}, inplace=True)\n",
    "all_test_features.rename(columns={'stay_id_static': 'stay_id'}, inplace=True)\n",
    "all_val_features.rename(columns={'stay_id_static': 'stay_id'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop not needed columns\n",
    "dynamic_train_data_norm.drop(columns='stay_id_x', inplace=True)\n",
    "dynamic_train_data_norm.drop(columns='timestep', inplace=True)\n",
    "\n",
    "dynamic_test_data_norm.drop(columns='stay_id_x', inplace=True)\n",
    "dynamic_test_data_norm.drop(columns='timestep', inplace=True)\n",
    "\n",
    "dynamic_val_data_norm.drop(columns='stay_id_x', inplace=True)\n",
    "dynamic_val_data_norm.drop(columns='timestep', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeated Static Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repeated Static Train Tensor shape: torch.Size([8326, 12, 2332])\n",
      "Repeated Static Tensor shape: torch.Size([1784, 12, 2332])\n",
      "Repeated Static Tensor shape: torch.Size([1785, 12, 2332])\n"
     ]
    }
   ],
   "source": [
    "# Reshape the data and capture stay_id order for all features\n",
    "all_features_train_array, train_stay_order = reshape_dynamic_data(all_train_features, num_time_steps)\n",
    "all_features_test_array, test_stay_order = reshape_dynamic_data(all_test_features, num_time_steps)\n",
    "all_features_val_array, val_stay_order = reshape_dynamic_data(all_val_features, num_time_steps)\n",
    "\n",
    "# Convert numpy arrays to tensors\n",
    "train_tensor_repeated_static_features = torch.tensor(all_features_train_array, dtype=torch.float32)\n",
    "test_tensor_repeated_static_features = torch.tensor(all_features_test_array, dtype=torch.float32)\n",
    "val_tensor_repeated_static_features = torch.tensor(all_features_val_array, dtype=torch.float32)\n",
    "\n",
    "# Print shapes to confirm\n",
    "print(\"Repeated Static Train Tensor shape:\", train_tensor_repeated_static_features.shape)  # Expected: [num_stays, 12, num_features]\n",
    "print(\"Repeated Static Tensor shape:\", test_tensor_repeated_static_features.shape)\n",
    "print(\"Repeated Static Tensor shape:\", val_tensor_repeated_static_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Repeated Static Tensors\n",
    "torch.save(train_tensor_repeated_static_features, os.path.join(tensor_save_path, 'train_tensor_repeated_static_features.pt'))\n",
    "torch.save(test_tensor_repeated_static_features, os.path.join(tensor_save_path, 'test_tensor_repeated_static_features.pt'))\n",
    "torch.save(val_tensor_repeated_static_features, os.path.join(tensor_save_path, 'val_tensor_repeated_static_features.pt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Feature Names for Feature Importance Computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and save feature names for Feature Importance Calculation\n",
    "feature_names_dynamic_features = dynamic_train_data_norm.columns\n",
    "\n",
    "# Save feature names\n",
    "feature_name_file = os.path.join(tensor_save_path, 'feature_names_dynamic_features.pkl')\n",
    "with open(feature_name_file, 'wb') as file:\n",
    "    pickle.dump(feature_names_dynamic_features, file)\n",
    "\n",
    "# Define Feature Names for Feature importance estimation\n",
    "feature_names_all_train_feaures=all_train_features.columns\n",
    "\n",
    "# Save feature names\n",
    "feature_name_file = os.path.join(tensor_save_path, 'feature_names_all_train_feaures.pkl')\n",
    "with open(feature_name_file, 'wb') as file:\n",
    "    pickle.dump(feature_names_all_train_feaures, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
